{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Step 1: Load Data\n",
    "data = pd.read_csv('features.csv')  # Replace with your dataset file name\n",
    "\n",
    "# Step 2: Drop 'report_date' and separate the target variable\n",
    "X = data.drop(columns=['report_date', 'risk_class'])  # Drop unnecessary columns\n",
    "y = data['risk_class']  # Target column\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)  # Convert 'High', 'Low', etc., to integers\n",
    "\n",
    "# Print the label encoding mapping\n",
    "print(\"Label Encoding Mapping:\")\n",
    "for idx, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{label}: {idx}\")\n",
    "\n",
    "# Step 3: Preprocessing - Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Feature Selection using Random Forest\n",
    "feature_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "feature_selector.fit(X_scaled, y)\n",
    "\n",
    "# Select top features based on importance\n",
    "important_features = np.where(feature_selector.feature_importances_ > 0.01)[0]\n",
    "X_selected = X_scaled[:, important_features]\n",
    "\n",
    "# Step 5: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Step 6: Model Training and Evaluation\n",
    "\n",
    "# Logistic Regression (Baseline)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "f1_log = f1_score(y_test, y_pred_log, average='weighted')\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBClassifier(n_estimators=500, learning_rate=0.01, max_depth=7, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')\n",
    "\n",
    "# TPOT (AutoML)\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=3, scoring='f1_weighted', random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "f1_tpot = f1_score(y_test, tpot.predict(X_test), average='weighted')\n",
    "\n",
    "# Step 7: Evaluation and Comparison\n",
    "print(f\"\\nF1 Scores:\")\n",
    "print(f\"Logistic Regression: {f1_log:.4f}\")\n",
    "print(f\"Random Forest: {f1_rf:.4f}\")\n",
    "print(f\"XGBoost: {f1_xgb:.4f}\")\n",
    "print(f\"TPOT: {f1_tpot:.4f}\")\n",
    "\n",
    "# Confusion Matrix Example\n",
    "print(\"\\nConfusion Matrix (XGBoost):\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "\n",
    "# Optional: Save selected features for interpretability\n",
    "selected_feature_names = data.columns[1:][important_features]\n",
    "selected_feature_names.to_csv('selected_features.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
